{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520f0c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6908a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_binary</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70687</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70688</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70689</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70690</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70691</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70692 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Diabetes_binary  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0                  0.0     1.0       0.0        1.0  26.0     0.0     0.0   \n",
       "1                  0.0     1.0       1.0        1.0  26.0     1.0     1.0   \n",
       "2                  0.0     0.0       0.0        1.0  26.0     0.0     0.0   \n",
       "3                  0.0     1.0       1.0        1.0  28.0     1.0     0.0   \n",
       "4                  0.0     0.0       0.0        1.0  29.0     1.0     0.0   \n",
       "...                ...     ...       ...        ...   ...     ...     ...   \n",
       "70687              1.0     0.0       1.0        1.0  37.0     0.0     0.0   \n",
       "70688              1.0     0.0       1.0        1.0  29.0     1.0     0.0   \n",
       "70689              1.0     1.0       1.0        1.0  25.0     0.0     0.0   \n",
       "70690              1.0     1.0       1.0        1.0  18.0     0.0     0.0   \n",
       "70691              1.0     1.0       1.0        1.0  25.0     0.0     0.0   \n",
       "\n",
       "       HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                       0.0           1.0     0.0  ...            1.0   \n",
       "1                       0.0           0.0     1.0  ...            1.0   \n",
       "2                       0.0           1.0     1.0  ...            1.0   \n",
       "3                       0.0           1.0     1.0  ...            1.0   \n",
       "4                       0.0           1.0     1.0  ...            1.0   \n",
       "...                     ...           ...     ...  ...            ...   \n",
       "70687                   0.0           0.0     0.0  ...            1.0   \n",
       "70688                   1.0           0.0     1.0  ...            1.0   \n",
       "70689                   1.0           0.0     1.0  ...            1.0   \n",
       "70690                   0.0           0.0     0.0  ...            1.0   \n",
       "70691                   1.0           1.0     1.0  ...            1.0   \n",
       "\n",
       "       NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  \\\n",
       "0              0.0      3.0       5.0      30.0       0.0  1.0   4.0   \n",
       "1              0.0      3.0       0.0       0.0       0.0  1.0  12.0   \n",
       "2              0.0      1.0       0.0      10.0       0.0  1.0  13.0   \n",
       "3              0.0      3.0       0.0       3.0       0.0  1.0  11.0   \n",
       "4              0.0      2.0       0.0       0.0       0.0  0.0   8.0   \n",
       "...            ...      ...       ...       ...       ...  ...   ...   \n",
       "70687          0.0      4.0       0.0       0.0       0.0  0.0   6.0   \n",
       "70688          0.0      2.0       0.0       0.0       1.0  1.0  10.0   \n",
       "70689          0.0      5.0      15.0       0.0       1.0  0.0  13.0   \n",
       "70690          0.0      4.0       0.0       0.0       1.0  0.0  11.0   \n",
       "70691          0.0      2.0       0.0       0.0       0.0  0.0   9.0   \n",
       "\n",
       "       Education  Income  \n",
       "0            6.0     8.0  \n",
       "1            6.0     8.0  \n",
       "2            6.0     8.0  \n",
       "3            6.0     8.0  \n",
       "4            5.0     8.0  \n",
       "...          ...     ...  \n",
       "70687        4.0     1.0  \n",
       "70688        3.0     6.0  \n",
       "70689        6.0     4.0  \n",
       "70690        2.0     4.0  \n",
       "70691        6.0     2.0  \n",
       "\n",
       "[70692 rows x 22 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv(\"diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3ae89c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_binary</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70687</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70688</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70689</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70690</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70691</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69057 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Diabetes_binary  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0                  0.0     1.0       0.0        1.0  26.0     0.0     0.0   \n",
       "1                  0.0     1.0       1.0        1.0  26.0     1.0     1.0   \n",
       "2                  0.0     0.0       0.0        1.0  26.0     0.0     0.0   \n",
       "3                  0.0     1.0       1.0        1.0  28.0     1.0     0.0   \n",
       "4                  0.0     0.0       0.0        1.0  29.0     1.0     0.0   \n",
       "...                ...     ...       ...        ...   ...     ...     ...   \n",
       "70687              1.0     0.0       1.0        1.0  37.0     0.0     0.0   \n",
       "70688              1.0     0.0       1.0        1.0  29.0     1.0     0.0   \n",
       "70689              1.0     1.0       1.0        1.0  25.0     0.0     0.0   \n",
       "70690              1.0     1.0       1.0        1.0  18.0     0.0     0.0   \n",
       "70691              1.0     1.0       1.0        1.0  25.0     0.0     0.0   \n",
       "\n",
       "       HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                       0.0           1.0     0.0  ...            1.0   \n",
       "1                       0.0           0.0     1.0  ...            1.0   \n",
       "2                       0.0           1.0     1.0  ...            1.0   \n",
       "3                       0.0           1.0     1.0  ...            1.0   \n",
       "4                       0.0           1.0     1.0  ...            1.0   \n",
       "...                     ...           ...     ...  ...            ...   \n",
       "70687                   0.0           0.0     0.0  ...            1.0   \n",
       "70688                   1.0           0.0     1.0  ...            1.0   \n",
       "70689                   1.0           0.0     1.0  ...            1.0   \n",
       "70690                   0.0           0.0     0.0  ...            1.0   \n",
       "70691                   1.0           1.0     1.0  ...            1.0   \n",
       "\n",
       "       NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  \\\n",
       "0              0.0      3.0       5.0      30.0       0.0  1.0   4.0   \n",
       "1              0.0      3.0       0.0       0.0       0.0  1.0  12.0   \n",
       "2              0.0      1.0       0.0      10.0       0.0  1.0  13.0   \n",
       "3              0.0      3.0       0.0       3.0       0.0  1.0  11.0   \n",
       "4              0.0      2.0       0.0       0.0       0.0  0.0   8.0   \n",
       "...            ...      ...       ...       ...       ...  ...   ...   \n",
       "70687          0.0      4.0       0.0       0.0       0.0  0.0   6.0   \n",
       "70688          0.0      2.0       0.0       0.0       1.0  1.0  10.0   \n",
       "70689          0.0      5.0      15.0       0.0       1.0  0.0  13.0   \n",
       "70690          0.0      4.0       0.0       0.0       1.0  0.0  11.0   \n",
       "70691          0.0      2.0       0.0       0.0       0.0  0.0   9.0   \n",
       "\n",
       "       Education  Income  \n",
       "0            6.0     8.0  \n",
       "1            6.0     8.0  \n",
       "2            6.0     8.0  \n",
       "3            6.0     8.0  \n",
       "4            5.0     8.0  \n",
       "...          ...     ...  \n",
       "70687        4.0     1.0  \n",
       "70688        3.0     6.0  \n",
       "70689        6.0     4.0  \n",
       "70690        2.0     4.0  \n",
       "70691        6.0     2.0  \n",
       "\n",
       "[69057 rows x 22 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop_duplicates()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9301233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (e.g., using median imputation for numerical columns)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df= pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a799c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\945875591.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['BMI'] = df2['BMI'].apply(temp)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\945875591.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['MentHlth'] = df2['MentHlth'].apply(temp)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\945875591.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['PhysHlth'] = df2['PhysHlth'].apply(temp)\n"
     ]
    }
   ],
   "source": [
    "# Handle outlier of BMI\n",
    "Q1 = df2[\"BMI\"].quantile(0.25)\n",
    "Q3 = df2[\"BMI\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound= Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "def temp(x):\n",
    "    if x > upper_bound:\n",
    "        return upper_bound  \n",
    "    elif x < lower_bound:\n",
    "        return lower_bound  \n",
    "    else:\n",
    "        return x \n",
    "df2['BMI'] = df2['BMI'].apply(temp)\n",
    "\n",
    "# handle outlier of MentHlth\n",
    "\n",
    "Q1 = df2[\"MentHlth\"].quantile(0.25)\n",
    "Q3 = df2[\"MentHlth\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound= Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "def temp(x):\n",
    "    if x > upper_bound:\n",
    "        return upper_bound  \n",
    "    elif x < lower_bound:\n",
    "        return lower_bound  \n",
    "    else:\n",
    "        return x \n",
    "df2['MentHlth'] = df2['MentHlth'].apply(temp)\n",
    "\n",
    "# handle outlier of PhysHlth\n",
    "\n",
    "Q1 = df2[\"PhysHlth\"].quantile(0.25)\n",
    "Q3 = df2[\"PhysHlth\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound= Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "def temp(x):\n",
    "    if x > upper_bound:\n",
    "        return upper_bound  \n",
    "    elif x < lower_bound:\n",
    "        return lower_bound  \n",
    "    else:\n",
    "        return x \n",
    "df2['PhysHlth'] = df2['PhysHlth'].apply(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "841688e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\878503397.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Metabolic_Risk'] = df2['HighBP'] + df2['HighChol'] + (df2['BMI'] > 30).astype(int)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\878503397.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Lifestyle_Score'] = df2['PhysActivity'] - df2['Smoker'] - (df2['Veggies'] < 1).astype(int)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\878503397.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Age_BMI_Interaction'] = df2['Age'] * (df2['BMI'] / 10)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\878503397.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['BP_Age'] = df2['HighBP'] * (df2['Age'] / 10)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_18796\\878503397.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Health_Risk'] = df2['Metabolic_Risk'] + (df2['GenHlth'] > 3).astype(int) + (df2['PhysHlth'] > 7).astype(int)\n"
     ]
    }
   ],
   "source": [
    "df2['Metabolic_Risk'] = df2['HighBP'] + df2['HighChol'] + (df2['BMI'] > 30).astype(int)\n",
    "df2['Lifestyle_Score'] = df2['PhysActivity'] - df2['Smoker'] - (df2['Veggies'] < 1).astype(int)\n",
    "df2['Age_BMI_Interaction'] = df2['Age'] * (df2['BMI'] / 10)\n",
    "df2['BP_Age'] = df2['HighBP'] * (df2['Age'] / 10)\n",
    "df2['Health_Risk'] = df2['Metabolic_Risk'] + (df2['GenHlth'] > 3).astype(int) + (df2['PhysHlth'] > 7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4984edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diabetes_binary',\n",
       " 'HighBP',\n",
       " 'HighChol',\n",
       " 'CholCheck',\n",
       " 'BMI',\n",
       " 'Smoker',\n",
       " 'Stroke',\n",
       " 'HeartDiseaseorAttack',\n",
       " 'PhysActivity',\n",
       " 'Fruits',\n",
       " 'Veggies',\n",
       " 'HvyAlcoholConsump',\n",
       " 'AnyHealthcare',\n",
       " 'NoDocbcCost',\n",
       " 'GenHlth',\n",
       " 'MentHlth',\n",
       " 'PhysHlth',\n",
       " 'DiffWalk',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'Education',\n",
       " 'Income',\n",
       " 'Metabolic_Risk',\n",
       " 'Lifestyle_Score',\n",
       " 'Age_BMI_Interaction',\n",
       " 'BP_Age',\n",
       " 'Health_Risk']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae7179d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2[\"Diabetes_binary\"].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0671bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee5a37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(columns=['Diabetes_binary'])\n",
    "y = df2['Diabetes_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cb75f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all features to float32 and ensure target is int\n",
    "X = X.astype('float32')\n",
    "y = y.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac120b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X , y, test_size=0.3, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8104a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f72e2de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.0167213528520949, 1: 0.9838197582122359}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weights = {i: w for i, w in zip(classes, weights)}\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "615f4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diabetes_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        InputLayer(shape=(input_shape,)),  # Correct way to specify input shape\n",
    "\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.Precision(name='precision')\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7bc2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7174 - loss: 0.5576 - precision: 0.7132 - recall: 0.7356 - val_accuracy: 0.7503 - val_loss: 0.5150 - val_precision: 0.7253 - val_recall: 0.8188\n",
      "Epoch 2/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.7474 - loss: 0.5108 - precision: 0.7309 - recall: 0.8040 - val_accuracy: 0.7493 - val_loss: 0.5113 - val_precision: 0.7389 - val_recall: 0.7836\n",
      "Epoch 3/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7556 - loss: 0.4993 - precision: 0.7383 - recall: 0.8029 - val_accuracy: 0.7502 - val_loss: 0.5101 - val_precision: 0.7326 - val_recall: 0.8008\n",
      "Epoch 4/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7534 - loss: 0.5046 - precision: 0.7348 - recall: 0.8018 - val_accuracy: 0.7500 - val_loss: 0.5103 - val_precision: 0.7212 - val_recall: 0.8282\n",
      "Epoch 5/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7531 - loss: 0.5045 - precision: 0.7327 - recall: 0.8085 - val_accuracy: 0.7509 - val_loss: 0.5091 - val_precision: 0.7294 - val_recall: 0.8106\n",
      "Epoch 6/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7517 - loss: 0.5066 - precision: 0.7332 - recall: 0.8073 - val_accuracy: 0.7496 - val_loss: 0.5095 - val_precision: 0.7306 - val_recall: 0.8037\n",
      "Epoch 7/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7495 - loss: 0.5069 - precision: 0.7313 - recall: 0.8029 - val_accuracy: 0.7524 - val_loss: 0.5095 - val_precision: 0.7307 - val_recall: 0.8121\n",
      "Epoch 8/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7537 - loss: 0.5013 - precision: 0.7338 - recall: 0.8089 - val_accuracy: 0.7483 - val_loss: 0.5098 - val_precision: 0.7388 - val_recall: 0.7809\n",
      "Epoch 9/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7597 - loss: 0.4947 - precision: 0.7412 - recall: 0.8133 - val_accuracy: 0.7486 - val_loss: 0.5092 - val_precision: 0.7297 - val_recall: 0.8026\n",
      "Epoch 10/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7563 - loss: 0.4988 - precision: 0.7387 - recall: 0.8101 - val_accuracy: 0.7494 - val_loss: 0.5103 - val_precision: 0.7218 - val_recall: 0.8251\n",
      "Epoch 11/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7543 - loss: 0.4989 - precision: 0.7352 - recall: 0.8083 - val_accuracy: 0.7493 - val_loss: 0.5100 - val_precision: 0.7276 - val_recall: 0.8099\n",
      "Epoch 12/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7551 - loss: 0.4997 - precision: 0.7361 - recall: 0.8071 - val_accuracy: 0.7505 - val_loss: 0.5102 - val_precision: 0.7265 - val_recall: 0.8163\n",
      "Epoch 13/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7586 - loss: 0.4956 - precision: 0.7399 - recall: 0.8158 - val_accuracy: 0.7493 - val_loss: 0.5106 - val_precision: 0.7259 - val_recall: 0.8140\n",
      "Epoch 14/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7571 - loss: 0.4959 - precision: 0.7377 - recall: 0.8083 - val_accuracy: 0.7490 - val_loss: 0.5103 - val_precision: 0.7304 - val_recall: 0.8022\n",
      "Epoch 15/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7614 - loss: 0.4930 - precision: 0.7436 - recall: 0.8120 - val_accuracy: 0.7501 - val_loss: 0.5110 - val_precision: 0.7261 - val_recall: 0.8162\n",
      "Epoch 16/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7582 - loss: 0.4969 - precision: 0.7396 - recall: 0.8096 - val_accuracy: 0.7458 - val_loss: 0.5127 - val_precision: 0.7392 - val_recall: 0.7723\n",
      "Epoch 17/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7625 - loss: 0.4901 - precision: 0.7436 - recall: 0.8154 - val_accuracy: 0.7469 - val_loss: 0.5116 - val_precision: 0.7364 - val_recall: 0.7818\n",
      "Epoch 18/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7619 - loss: 0.4913 - precision: 0.7459 - recall: 0.8077 - val_accuracy: 0.7469 - val_loss: 0.5146 - val_precision: 0.7139 - val_recall: 0.8377\n",
      "Epoch 19/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7609 - loss: 0.4898 - precision: 0.7403 - recall: 0.8192 - val_accuracy: 0.7486 - val_loss: 0.5117 - val_precision: 0.7351 - val_recall: 0.7902\n",
      "Epoch 20/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7629 - loss: 0.4880 - precision: 0.7446 - recall: 0.8109 - val_accuracy: 0.7480 - val_loss: 0.5135 - val_precision: 0.7221 - val_recall: 0.8196\n",
      "Epoch 21/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7645 - loss: 0.4885 - precision: 0.7460 - recall: 0.8162 - val_accuracy: 0.7476 - val_loss: 0.5130 - val_precision: 0.7284 - val_recall: 0.8026\n",
      "Epoch 22/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7644 - loss: 0.4821 - precision: 0.7445 - recall: 0.8186 - val_accuracy: 0.7463 - val_loss: 0.5129 - val_precision: 0.7344 - val_recall: 0.7844\n",
      "Epoch 23/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7666 - loss: 0.4835 - precision: 0.7461 - recall: 0.8165 - val_accuracy: 0.7464 - val_loss: 0.5137 - val_precision: 0.7303 - val_recall: 0.7944\n",
      "Epoch 24/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7618 - loss: 0.4889 - precision: 0.7442 - recall: 0.8145 - val_accuracy: 0.7473 - val_loss: 0.5139 - val_precision: 0.7311 - val_recall: 0.7953\n",
      "Epoch 25/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7664 - loss: 0.4814 - precision: 0.7485 - recall: 0.8149 - val_accuracy: 0.7463 - val_loss: 0.5147 - val_precision: 0.7289 - val_recall: 0.7975\n",
      "Epoch 26/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7614 - loss: 0.4844 - precision: 0.7415 - recall: 0.8109 - val_accuracy: 0.7462 - val_loss: 0.5157 - val_precision: 0.7389 - val_recall: 0.7744\n",
      "Epoch 27/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7654 - loss: 0.4845 - precision: 0.7469 - recall: 0.8116 - val_accuracy: 0.7457 - val_loss: 0.5164 - val_precision: 0.7171 - val_recall: 0.8252\n",
      "Epoch 28/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7666 - loss: 0.4845 - precision: 0.7478 - recall: 0.8168 - val_accuracy: 0.7460 - val_loss: 0.5151 - val_precision: 0.7297 - val_recall: 0.7944\n",
      "Epoch 29/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7683 - loss: 0.4778 - precision: 0.7510 - recall: 0.8151 - val_accuracy: 0.7435 - val_loss: 0.5170 - val_precision: 0.7366 - val_recall: 0.7710\n",
      "Epoch 30/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7661 - loss: 0.4802 - precision: 0.7455 - recall: 0.8142 - val_accuracy: 0.7459 - val_loss: 0.5169 - val_precision: 0.7219 - val_recall: 0.8134\n",
      "Epoch 31/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7651 - loss: 0.4840 - precision: 0.7455 - recall: 0.8151 - val_accuracy: 0.7444 - val_loss: 0.5170 - val_precision: 0.7317 - val_recall: 0.7848\n",
      "Epoch 32/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7690 - loss: 0.4807 - precision: 0.7517 - recall: 0.8141 - val_accuracy: 0.7437 - val_loss: 0.5197 - val_precision: 0.7135 - val_recall: 0.8286\n",
      "Epoch 33/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7673 - loss: 0.4790 - precision: 0.7475 - recall: 0.8181 - val_accuracy: 0.7465 - val_loss: 0.5188 - val_precision: 0.7241 - val_recall: 0.8097\n",
      "Epoch 34/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7665 - loss: 0.4793 - precision: 0.7501 - recall: 0.8167 - val_accuracy: 0.7431 - val_loss: 0.5192 - val_precision: 0.7340 - val_recall: 0.7755\n",
      "Epoch 35/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7698 - loss: 0.4772 - precision: 0.7536 - recall: 0.8118 - val_accuracy: 0.7450 - val_loss: 0.5198 - val_precision: 0.7211 - val_recall: 0.8125\n",
      "Epoch 36/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7696 - loss: 0.4774 - precision: 0.7510 - recall: 0.8189 - val_accuracy: 0.7434 - val_loss: 0.5201 - val_precision: 0.7270 - val_recall: 0.7929\n",
      "Epoch 37/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7696 - loss: 0.4775 - precision: 0.7550 - recall: 0.8170 - val_accuracy: 0.7431 - val_loss: 0.5207 - val_precision: 0.7262 - val_recall: 0.7940\n",
      "Epoch 38/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7695 - loss: 0.4754 - precision: 0.7540 - recall: 0.8168 - val_accuracy: 0.7448 - val_loss: 0.5214 - val_precision: 0.7325 - val_recall: 0.7842\n",
      "Epoch 39/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7706 - loss: 0.4758 - precision: 0.7530 - recall: 0.8162 - val_accuracy: 0.7412 - val_loss: 0.5221 - val_precision: 0.7286 - val_recall: 0.7821\n",
      "Epoch 40/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7702 - loss: 0.4757 - precision: 0.7523 - recall: 0.8149 - val_accuracy: 0.7423 - val_loss: 0.5241 - val_precision: 0.7334 - val_recall: 0.7746\n",
      "Epoch 41/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7733 - loss: 0.4731 - precision: 0.7547 - recall: 0.8183 - val_accuracy: 0.7433 - val_loss: 0.5236 - val_precision: 0.7286 - val_recall: 0.7889\n",
      "Epoch 42/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7732 - loss: 0.4708 - precision: 0.7563 - recall: 0.8200 - val_accuracy: 0.7414 - val_loss: 0.5245 - val_precision: 0.7310 - val_recall: 0.7772\n",
      "Epoch 43/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7726 - loss: 0.4716 - precision: 0.7531 - recall: 0.8154 - val_accuracy: 0.7411 - val_loss: 0.5242 - val_precision: 0.7267 - val_recall: 0.7865\n",
      "Epoch 44/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7732 - loss: 0.4679 - precision: 0.7573 - recall: 0.8118 - val_accuracy: 0.7409 - val_loss: 0.5255 - val_precision: 0.7248 - val_recall: 0.7901\n",
      "Epoch 45/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7738 - loss: 0.4695 - precision: 0.7565 - recall: 0.8201 - val_accuracy: 0.7417 - val_loss: 0.5254 - val_precision: 0.7231 - val_recall: 0.7970\n",
      "Epoch 46/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7745 - loss: 0.4665 - precision: 0.7579 - recall: 0.8166 - val_accuracy: 0.7404 - val_loss: 0.5277 - val_precision: 0.7293 - val_recall: 0.7780\n",
      "Epoch 47/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7781 - loss: 0.4647 - precision: 0.7607 - recall: 0.8224 - val_accuracy: 0.7400 - val_loss: 0.5282 - val_precision: 0.7193 - val_recall: 0.8010\n",
      "Epoch 48/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7756 - loss: 0.4624 - precision: 0.7563 - recall: 0.8194 - val_accuracy: 0.7407 - val_loss: 0.5281 - val_precision: 0.7210 - val_recall: 0.7990\n",
      "Epoch 49/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7747 - loss: 0.4667 - precision: 0.7564 - recall: 0.8212 - val_accuracy: 0.7412 - val_loss: 0.5295 - val_precision: 0.7261 - val_recall: 0.7881\n",
      "Epoch 50/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7813 - loss: 0.4598 - precision: 0.7654 - recall: 0.8214 - val_accuracy: 0.7390 - val_loss: 0.5299 - val_precision: 0.7293 - val_recall: 0.7736\n",
      "Epoch 51/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7746 - loss: 0.4655 - precision: 0.7596 - recall: 0.8156 - val_accuracy: 0.7399 - val_loss: 0.5331 - val_precision: 0.7286 - val_recall: 0.7782\n",
      "Epoch 52/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7786 - loss: 0.4613 - precision: 0.7627 - recall: 0.8220 - val_accuracy: 0.7405 - val_loss: 0.5324 - val_precision: 0.7219 - val_recall: 0.7960\n",
      "Epoch 53/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7766 - loss: 0.4632 - precision: 0.7599 - recall: 0.8170 - val_accuracy: 0.7379 - val_loss: 0.5325 - val_precision: 0.7152 - val_recall: 0.8047\n",
      "Epoch 54/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7789 - loss: 0.4622 - precision: 0.7613 - recall: 0.8197 - val_accuracy: 0.7394 - val_loss: 0.5336 - val_precision: 0.7313 - val_recall: 0.7703\n",
      "Epoch 55/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7791 - loss: 0.4611 - precision: 0.7637 - recall: 0.8251 - val_accuracy: 0.7377 - val_loss: 0.5338 - val_precision: 0.7306 - val_recall: 0.7666\n",
      "Epoch 56/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7799 - loss: 0.4595 - precision: 0.7641 - recall: 0.8183 - val_accuracy: 0.7367 - val_loss: 0.5351 - val_precision: 0.7165 - val_recall: 0.7973\n",
      "Epoch 57/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7817 - loss: 0.4578 - precision: 0.7644 - recall: 0.8276 - val_accuracy: 0.7366 - val_loss: 0.5362 - val_precision: 0.7299 - val_recall: 0.7647\n",
      "Epoch 58/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7789 - loss: 0.4603 - precision: 0.7652 - recall: 0.8167 - val_accuracy: 0.7368 - val_loss: 0.5368 - val_precision: 0.7311 - val_recall: 0.7625\n",
      "Epoch 59/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7796 - loss: 0.4572 - precision: 0.7642 - recall: 0.8130 - val_accuracy: 0.7344 - val_loss: 0.5382 - val_precision: 0.7131 - val_recall: 0.7988\n",
      "Epoch 60/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7798 - loss: 0.4590 - precision: 0.7623 - recall: 0.8213 - val_accuracy: 0.7377 - val_loss: 0.5377 - val_precision: 0.7205 - val_recall: 0.7904\n",
      "Epoch 61/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7820 - loss: 0.4557 - precision: 0.7679 - recall: 0.8191 - val_accuracy: 0.7304 - val_loss: 0.5406 - val_precision: 0.7315 - val_recall: 0.7417\n",
      "Epoch 62/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7846 - loss: 0.4523 - precision: 0.7687 - recall: 0.8223 - val_accuracy: 0.7329 - val_loss: 0.5413 - val_precision: 0.7329 - val_recall: 0.7467\n",
      "Epoch 63/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7830 - loss: 0.4558 - precision: 0.7679 - recall: 0.8199 - val_accuracy: 0.7366 - val_loss: 0.5410 - val_precision: 0.7215 - val_recall: 0.7844\n",
      "Epoch 64/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7826 - loss: 0.4525 - precision: 0.7696 - recall: 0.8204 - val_accuracy: 0.7346 - val_loss: 0.5413 - val_precision: 0.7223 - val_recall: 0.7763\n",
      "Epoch 65/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7851 - loss: 0.4524 - precision: 0.7689 - recall: 0.8235 - val_accuracy: 0.7368 - val_loss: 0.5433 - val_precision: 0.7221 - val_recall: 0.7839\n",
      "Epoch 66/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7826 - loss: 0.4531 - precision: 0.7684 - recall: 0.8173 - val_accuracy: 0.7352 - val_loss: 0.5429 - val_precision: 0.7269 - val_recall: 0.7673\n",
      "Epoch 67/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7849 - loss: 0.4492 - precision: 0.7681 - recall: 0.8229 - val_accuracy: 0.7340 - val_loss: 0.5433 - val_precision: 0.7214 - val_recall: 0.7764\n",
      "Epoch 68/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7823 - loss: 0.4509 - precision: 0.7693 - recall: 0.8222 - val_accuracy: 0.7347 - val_loss: 0.5439 - val_precision: 0.7210 - val_recall: 0.7796\n",
      "Epoch 69/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7881 - loss: 0.4495 - precision: 0.7741 - recall: 0.8233 - val_accuracy: 0.7334 - val_loss: 0.5460 - val_precision: 0.7157 - val_recall: 0.7889\n",
      "Epoch 70/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7849 - loss: 0.4489 - precision: 0.7701 - recall: 0.8236 - val_accuracy: 0.7353 - val_loss: 0.5458 - val_precision: 0.7236 - val_recall: 0.7752\n",
      "Epoch 71/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7854 - loss: 0.4490 - precision: 0.7691 - recall: 0.8239 - val_accuracy: 0.7352 - val_loss: 0.5478 - val_precision: 0.7208 - val_recall: 0.7819\n",
      "Epoch 72/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7836 - loss: 0.4497 - precision: 0.7686 - recall: 0.8225 - val_accuracy: 0.7350 - val_loss: 0.5484 - val_precision: 0.7172 - val_recall: 0.7903\n",
      "Epoch 73/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7846 - loss: 0.4484 - precision: 0.7697 - recall: 0.8203 - val_accuracy: 0.7293 - val_loss: 0.5507 - val_precision: 0.7332 - val_recall: 0.7349\n",
      "Epoch 74/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7863 - loss: 0.4472 - precision: 0.7729 - recall: 0.8177 - val_accuracy: 0.7322 - val_loss: 0.5496 - val_precision: 0.7241 - val_recall: 0.7641\n",
      "Epoch 75/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7894 - loss: 0.4443 - precision: 0.7745 - recall: 0.8261 - val_accuracy: 0.7332 - val_loss: 0.5523 - val_precision: 0.7240 - val_recall: 0.7676\n",
      "Epoch 76/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7862 - loss: 0.4430 - precision: 0.7717 - recall: 0.8224 - val_accuracy: 0.7317 - val_loss: 0.5521 - val_precision: 0.7153 - val_recall: 0.7842\n",
      "Epoch 77/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7860 - loss: 0.4441 - precision: 0.7717 - recall: 0.8256 - val_accuracy: 0.7300 - val_loss: 0.5525 - val_precision: 0.7314 - val_recall: 0.7409\n",
      "Epoch 78/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7873 - loss: 0.4439 - precision: 0.7730 - recall: 0.8215 - val_accuracy: 0.7318 - val_loss: 0.5538 - val_precision: 0.7202 - val_recall: 0.7724\n",
      "Epoch 79/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7858 - loss: 0.4443 - precision: 0.7696 - recall: 0.8238 - val_accuracy: 0.7310 - val_loss: 0.5552 - val_precision: 0.7256 - val_recall: 0.7568\n",
      "Epoch 80/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7892 - loss: 0.4415 - precision: 0.7756 - recall: 0.8231 - val_accuracy: 0.7333 - val_loss: 0.5553 - val_precision: 0.7211 - val_recall: 0.7750\n",
      "Epoch 81/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7934 - loss: 0.4362 - precision: 0.7780 - recall: 0.8265 - val_accuracy: 0.7278 - val_loss: 0.5565 - val_precision: 0.7305 - val_recall: 0.7361\n",
      "Epoch 82/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7901 - loss: 0.4381 - precision: 0.7756 - recall: 0.8254 - val_accuracy: 0.7263 - val_loss: 0.5604 - val_precision: 0.7027 - val_recall: 0.7998\n",
      "Epoch 83/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7932 - loss: 0.4386 - precision: 0.7774 - recall: 0.8287 - val_accuracy: 0.7288 - val_loss: 0.5607 - val_precision: 0.7330 - val_recall: 0.7335\n",
      "Epoch 84/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7895 - loss: 0.4387 - precision: 0.7770 - recall: 0.8192 - val_accuracy: 0.7275 - val_loss: 0.5585 - val_precision: 0.7180 - val_recall: 0.7638\n",
      "Epoch 85/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7895 - loss: 0.4384 - precision: 0.7772 - recall: 0.8242 - val_accuracy: 0.7304 - val_loss: 0.5590 - val_precision: 0.7204 - val_recall: 0.7674\n",
      "Epoch 86/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7875 - loss: 0.4406 - precision: 0.7750 - recall: 0.8213 - val_accuracy: 0.7298 - val_loss: 0.5601 - val_precision: 0.7248 - val_recall: 0.7549\n",
      "Epoch 87/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7912 - loss: 0.4381 - precision: 0.7785 - recall: 0.8265 - val_accuracy: 0.7244 - val_loss: 0.5624 - val_precision: 0.7203 - val_recall: 0.7483\n",
      "Epoch 88/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7943 - loss: 0.4338 - precision: 0.7796 - recall: 0.8301 - val_accuracy: 0.7296 - val_loss: 0.5626 - val_precision: 0.7203 - val_recall: 0.7650\n",
      "Epoch 89/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7928 - loss: 0.4395 - precision: 0.7778 - recall: 0.8275 - val_accuracy: 0.7300 - val_loss: 0.5625 - val_precision: 0.7242 - val_recall: 0.7570\n",
      "Epoch 90/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7960 - loss: 0.4340 - precision: 0.7820 - recall: 0.8291 - val_accuracy: 0.7270 - val_loss: 0.5635 - val_precision: 0.7157 - val_recall: 0.7680\n",
      "Epoch 91/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7936 - loss: 0.4358 - precision: 0.7813 - recall: 0.8230 - val_accuracy: 0.7294 - val_loss: 0.5652 - val_precision: 0.7190 - val_recall: 0.7675\n",
      "Epoch 92/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7979 - loss: 0.4304 - precision: 0.7843 - recall: 0.8267 - val_accuracy: 0.7283 - val_loss: 0.5664 - val_precision: 0.7071 - val_recall: 0.7943\n",
      "Epoch 93/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7969 - loss: 0.4284 - precision: 0.7840 - recall: 0.8286 - val_accuracy: 0.7230 - val_loss: 0.5663 - val_precision: 0.7182 - val_recall: 0.7490\n",
      "Epoch 94/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7953 - loss: 0.4325 - precision: 0.7815 - recall: 0.8261 - val_accuracy: 0.7298 - val_loss: 0.5687 - val_precision: 0.7147 - val_recall: 0.7796\n",
      "Epoch 95/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7942 - loss: 0.4316 - precision: 0.7808 - recall: 0.8289 - val_accuracy: 0.7258 - val_loss: 0.5688 - val_precision: 0.7277 - val_recall: 0.7358\n",
      "Epoch 96/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7944 - loss: 0.4301 - precision: 0.7808 - recall: 0.8218 - val_accuracy: 0.7279 - val_loss: 0.5692 - val_precision: 0.7219 - val_recall: 0.7557\n",
      "Epoch 97/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7981 - loss: 0.4289 - precision: 0.7820 - recall: 0.8310 - val_accuracy: 0.7251 - val_loss: 0.5719 - val_precision: 0.7283 - val_recall: 0.7323\n",
      "Epoch 98/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7963 - loss: 0.4322 - precision: 0.7845 - recall: 0.8276 - val_accuracy: 0.7224 - val_loss: 0.5719 - val_precision: 0.7051 - val_recall: 0.7801\n",
      "Epoch 99/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7961 - loss: 0.4285 - precision: 0.7830 - recall: 0.8292 - val_accuracy: 0.7253 - val_loss: 0.5731 - val_precision: 0.7271 - val_recall: 0.7354\n",
      "Epoch 100/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7979 - loss: 0.4272 - precision: 0.7850 - recall: 0.8320 - val_accuracy: 0.7236 - val_loss: 0.5774 - val_precision: 0.6961 - val_recall: 0.8098\n",
      "Epoch 101/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7992 - loss: 0.4291 - precision: 0.7854 - recall: 0.8320 - val_accuracy: 0.7211 - val_loss: 0.5765 - val_precision: 0.7306 - val_recall: 0.7148\n",
      "Epoch 102/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7972 - loss: 0.4279 - precision: 0.7855 - recall: 0.8245 - val_accuracy: 0.7281 - val_loss: 0.5768 - val_precision: 0.7058 - val_recall: 0.7975\n",
      "Epoch 103/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7990 - loss: 0.4236 - precision: 0.7873 - recall: 0.8278 - val_accuracy: 0.7248 - val_loss: 0.5734 - val_precision: 0.7217 - val_recall: 0.7462\n",
      "Epoch 104/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7987 - loss: 0.4259 - precision: 0.7854 - recall: 0.8306 - val_accuracy: 0.7275 - val_loss: 0.5751 - val_precision: 0.7213 - val_recall: 0.7559\n",
      "Epoch 105/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7985 - loss: 0.4277 - precision: 0.7852 - recall: 0.8328 - val_accuracy: 0.7254 - val_loss: 0.5770 - val_precision: 0.7195 - val_recall: 0.7534\n",
      "Epoch 106/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7981 - loss: 0.4267 - precision: 0.7849 - recall: 0.8308 - val_accuracy: 0.7218 - val_loss: 0.5762 - val_precision: 0.7131 - val_recall: 0.7575\n",
      "Epoch 107/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8004 - loss: 0.4235 - precision: 0.7869 - recall: 0.8330 - val_accuracy: 0.7256 - val_loss: 0.5789 - val_precision: 0.7195 - val_recall: 0.7540\n",
      "Epoch 108/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8007 - loss: 0.4227 - precision: 0.7866 - recall: 0.8288 - val_accuracy: 0.7218 - val_loss: 0.5795 - val_precision: 0.7083 - val_recall: 0.7698\n",
      "Epoch 109/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8026 - loss: 0.4222 - precision: 0.7908 - recall: 0.8311 - val_accuracy: 0.7230 - val_loss: 0.5780 - val_precision: 0.7140 - val_recall: 0.7591\n",
      "Epoch 110/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8004 - loss: 0.4215 - precision: 0.7875 - recall: 0.8323 - val_accuracy: 0.7233 - val_loss: 0.5823 - val_precision: 0.7041 - val_recall: 0.7858\n",
      "Epoch 111/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8003 - loss: 0.4234 - precision: 0.7875 - recall: 0.8332 - val_accuracy: 0.7234 - val_loss: 0.5822 - val_precision: 0.7104 - val_recall: 0.7696\n",
      "Epoch 112/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8026 - loss: 0.4207 - precision: 0.7913 - recall: 0.8331 - val_accuracy: 0.7229 - val_loss: 0.5808 - val_precision: 0.7109 - val_recall: 0.7668\n",
      "Epoch 113/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7997 - loss: 0.4227 - precision: 0.7885 - recall: 0.8271 - val_accuracy: 0.7245 - val_loss: 0.5812 - val_precision: 0.7154 - val_recall: 0.7604\n",
      "Epoch 114/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8012 - loss: 0.4186 - precision: 0.7891 - recall: 0.8303 - val_accuracy: 0.7243 - val_loss: 0.5867 - val_precision: 0.7225 - val_recall: 0.7428\n",
      "Epoch 115/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8032 - loss: 0.4194 - precision: 0.7898 - recall: 0.8343 - val_accuracy: 0.7226 - val_loss: 0.5883 - val_precision: 0.7006 - val_recall: 0.7931\n",
      "Epoch 116/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8039 - loss: 0.4189 - precision: 0.7917 - recall: 0.8317 - val_accuracy: 0.7254 - val_loss: 0.5873 - val_precision: 0.7203 - val_recall: 0.7515\n",
      "Epoch 117/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8077 - loss: 0.4141 - precision: 0.7951 - recall: 0.8349 - val_accuracy: 0.7187 - val_loss: 0.5895 - val_precision: 0.7031 - val_recall: 0.7730\n",
      "Epoch 118/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8031 - loss: 0.4179 - precision: 0.7909 - recall: 0.8281 - val_accuracy: 0.7245 - val_loss: 0.5861 - val_precision: 0.7202 - val_recall: 0.7491\n",
      "Epoch 119/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8031 - loss: 0.4176 - precision: 0.7903 - recall: 0.8338 - val_accuracy: 0.7172 - val_loss: 0.5887 - val_precision: 0.7257 - val_recall: 0.7130\n",
      "Epoch 120/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8013 - loss: 0.4191 - precision: 0.7897 - recall: 0.8277 - val_accuracy: 0.7192 - val_loss: 0.5888 - val_precision: 0.7154 - val_recall: 0.7432\n",
      "Epoch 121/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8045 - loss: 0.4187 - precision: 0.7913 - recall: 0.8333 - val_accuracy: 0.7229 - val_loss: 0.5904 - val_precision: 0.7188 - val_recall: 0.7471\n",
      "Epoch 122/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.4148 - precision: 0.7941 - recall: 0.8327 - val_accuracy: 0.7258 - val_loss: 0.5909 - val_precision: 0.7108 - val_recall: 0.7764\n",
      "Epoch 123/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8007 - loss: 0.4210 - precision: 0.7877 - recall: 0.8307 - val_accuracy: 0.7223 - val_loss: 0.5899 - val_precision: 0.7186 - val_recall: 0.7457\n",
      "Epoch 124/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.4176 - precision: 0.7927 - recall: 0.8350 - val_accuracy: 0.7244 - val_loss: 0.5909 - val_precision: 0.7156 - val_recall: 0.7597\n",
      "Epoch 125/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8031 - loss: 0.4144 - precision: 0.7916 - recall: 0.8279 - val_accuracy: 0.7204 - val_loss: 0.5927 - val_precision: 0.7073 - val_recall: 0.7673\n",
      "Epoch 126/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8070 - loss: 0.4158 - precision: 0.7950 - recall: 0.8348 - val_accuracy: 0.7219 - val_loss: 0.5969 - val_precision: 0.7072 - val_recall: 0.7726\n",
      "Epoch 127/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8056 - loss: 0.4125 - precision: 0.7947 - recall: 0.8330 - val_accuracy: 0.7163 - val_loss: 0.5963 - val_precision: 0.7027 - val_recall: 0.7660\n",
      "Epoch 128/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8058 - loss: 0.4120 - precision: 0.7946 - recall: 0.8350 - val_accuracy: 0.7205 - val_loss: 0.5955 - val_precision: 0.7152 - val_recall: 0.7479\n",
      "Epoch 129/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8040 - loss: 0.4153 - precision: 0.7940 - recall: 0.8329 - val_accuracy: 0.7201 - val_loss: 0.5972 - val_precision: 0.7117 - val_recall: 0.7555\n",
      "Epoch 130/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8050 - loss: 0.4153 - precision: 0.7919 - recall: 0.8354 - val_accuracy: 0.7173 - val_loss: 0.5971 - val_precision: 0.7151 - val_recall: 0.7377\n",
      "Epoch 131/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8091 - loss: 0.4096 - precision: 0.7983 - recall: 0.8348 - val_accuracy: 0.7211 - val_loss: 0.5985 - val_precision: 0.7214 - val_recall: 0.7352\n",
      "Epoch 132/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8089 - loss: 0.4104 - precision: 0.7968 - recall: 0.8359 - val_accuracy: 0.7217 - val_loss: 0.5990 - val_precision: 0.7154 - val_recall: 0.7512\n",
      "Epoch 133/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8098 - loss: 0.4106 - precision: 0.7980 - recall: 0.8403 - val_accuracy: 0.7194 - val_loss: 0.5996 - val_precision: 0.7058 - val_recall: 0.7681\n",
      "Epoch 134/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8074 - loss: 0.4118 - precision: 0.7977 - recall: 0.8349 - val_accuracy: 0.7160 - val_loss: 0.6034 - val_precision: 0.7223 - val_recall: 0.7167\n",
      "Epoch 135/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8050 - loss: 0.4114 - precision: 0.7972 - recall: 0.8260 - val_accuracy: 0.7146 - val_loss: 0.6028 - val_precision: 0.7092 - val_recall: 0.7433\n",
      "Epoch 136/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8088 - loss: 0.4073 - precision: 0.7982 - recall: 0.8371 - val_accuracy: 0.7204 - val_loss: 0.6048 - val_precision: 0.7200 - val_recall: 0.7363\n",
      "Epoch 137/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8091 - loss: 0.4076 - precision: 0.7988 - recall: 0.8365 - val_accuracy: 0.7193 - val_loss: 0.6061 - val_precision: 0.7249 - val_recall: 0.7215\n",
      "Epoch 138/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8128 - loss: 0.4043 - precision: 0.8032 - recall: 0.8399 - val_accuracy: 0.7132 - val_loss: 0.6061 - val_precision: 0.7167 - val_recall: 0.7204\n",
      "Epoch 139/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8124 - loss: 0.4046 - precision: 0.8010 - recall: 0.8392 - val_accuracy: 0.7118 - val_loss: 0.6064 - val_precision: 0.7183 - val_recall: 0.7125\n",
      "Epoch 140/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8110 - loss: 0.4044 - precision: 0.8017 - recall: 0.8353 - val_accuracy: 0.7217 - val_loss: 0.6073 - val_precision: 0.7114 - val_recall: 0.7613\n",
      "Epoch 141/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8131 - loss: 0.4039 - precision: 0.8018 - recall: 0.8422 - val_accuracy: 0.7164 - val_loss: 0.6069 - val_precision: 0.7038 - val_recall: 0.7632\n",
      "Epoch 142/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8117 - loss: 0.4078 - precision: 0.8003 - recall: 0.8411 - val_accuracy: 0.7142 - val_loss: 0.6097 - val_precision: 0.7031 - val_recall: 0.7575\n",
      "Epoch 143/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8107 - loss: 0.4045 - precision: 0.8005 - recall: 0.8390 - val_accuracy: 0.7157 - val_loss: 0.6092 - val_precision: 0.7203 - val_recall: 0.7203\n",
      "Epoch 144/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.4028 - precision: 0.8007 - recall: 0.8368 - val_accuracy: 0.7127 - val_loss: 0.6142 - val_precision: 0.7071 - val_recall: 0.7423\n",
      "Epoch 145/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8096 - loss: 0.4070 - precision: 0.8009 - recall: 0.8366 - val_accuracy: 0.7162 - val_loss: 0.6071 - val_precision: 0.7116 - val_recall: 0.7427\n",
      "Epoch 146/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8150 - loss: 0.4021 - precision: 0.8045 - recall: 0.8392 - val_accuracy: 0.7204 - val_loss: 0.6123 - val_precision: 0.7061 - val_recall: 0.7708\n",
      "Epoch 147/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8122 - loss: 0.4039 - precision: 0.8026 - recall: 0.8386 - val_accuracy: 0.7173 - val_loss: 0.6116 - val_precision: 0.7132 - val_recall: 0.7422\n",
      "Epoch 148/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8087 - loss: 0.4052 - precision: 0.7995 - recall: 0.8312 - val_accuracy: 0.7106 - val_loss: 0.6159 - val_precision: 0.6990 - val_recall: 0.7563\n",
      "Epoch 149/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8136 - loss: 0.3997 - precision: 0.8043 - recall: 0.8375 - val_accuracy: 0.7192 - val_loss: 0.6160 - val_precision: 0.7177 - val_recall: 0.7375\n",
      "Epoch 150/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8140 - loss: 0.3986 - precision: 0.8059 - recall: 0.8339 - val_accuracy: 0.7190 - val_loss: 0.6178 - val_precision: 0.7193 - val_recall: 0.7332\n",
      "Epoch 151/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8145 - loss: 0.4005 - precision: 0.8053 - recall: 0.8402 - val_accuracy: 0.7142 - val_loss: 0.6179 - val_precision: 0.7187 - val_recall: 0.7191\n",
      "Epoch 152/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8153 - loss: 0.3984 - precision: 0.8057 - recall: 0.8413 - val_accuracy: 0.7150 - val_loss: 0.6174 - val_precision: 0.7065 - val_recall: 0.7515\n",
      "Epoch 153/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8147 - loss: 0.3959 - precision: 0.8056 - recall: 0.8397 - val_accuracy: 0.7137 - val_loss: 0.6168 - val_precision: 0.7073 - val_recall: 0.7448\n",
      "Epoch 154/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8149 - loss: 0.3979 - precision: 0.8056 - recall: 0.8432 - val_accuracy: 0.7136 - val_loss: 0.6241 - val_precision: 0.7001 - val_recall: 0.7634\n",
      "Epoch 155/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8131 - loss: 0.4006 - precision: 0.8059 - recall: 0.8344 - val_accuracy: 0.7122 - val_loss: 0.6196 - val_precision: 0.7093 - val_recall: 0.7349\n",
      "Epoch 156/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8119 - loss: 0.4011 - precision: 0.8022 - recall: 0.8362 - val_accuracy: 0.7180 - val_loss: 0.6202 - val_precision: 0.7077 - val_recall: 0.7584\n",
      "Epoch 157/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8147 - loss: 0.3979 - precision: 0.8052 - recall: 0.8389 - val_accuracy: 0.7116 - val_loss: 0.6224 - val_precision: 0.6978 - val_recall: 0.7632\n",
      "Epoch 158/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8167 - loss: 0.3935 - precision: 0.8089 - recall: 0.8409 - val_accuracy: 0.7108 - val_loss: 0.6231 - val_precision: 0.7094 - val_recall: 0.7300\n",
      "Epoch 159/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8167 - loss: 0.3951 - precision: 0.8075 - recall: 0.8391 - val_accuracy: 0.7137 - val_loss: 0.6227 - val_precision: 0.7077 - val_recall: 0.7440\n",
      "Epoch 160/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8134 - loss: 0.3997 - precision: 0.8039 - recall: 0.8366 - val_accuracy: 0.7104 - val_loss: 0.6229 - val_precision: 0.7143 - val_recall: 0.7172\n",
      "Epoch 161/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8161 - loss: 0.3983 - precision: 0.8050 - recall: 0.8401 - val_accuracy: 0.7141 - val_loss: 0.6260 - val_precision: 0.7176 - val_recall: 0.7215\n",
      "Epoch 162/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8151 - loss: 0.3984 - precision: 0.8053 - recall: 0.8414 - val_accuracy: 0.7152 - val_loss: 0.6235 - val_precision: 0.7141 - val_recall: 0.7332\n",
      "Epoch 163/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8147 - loss: 0.3969 - precision: 0.8047 - recall: 0.8387 - val_accuracy: 0.7129 - val_loss: 0.6307 - val_precision: 0.7173 - val_recall: 0.7181\n",
      "Epoch 164/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8184 - loss: 0.3938 - precision: 0.8095 - recall: 0.8388 - val_accuracy: 0.7145 - val_loss: 0.6254 - val_precision: 0.7081 - val_recall: 0.7456\n",
      "Epoch 165/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8173 - loss: 0.3920 - precision: 0.8087 - recall: 0.8395 - val_accuracy: 0.7113 - val_loss: 0.6283 - val_precision: 0.7136 - val_recall: 0.7214\n",
      "Epoch 166/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8191 - loss: 0.3962 - precision: 0.8106 - recall: 0.8407 - val_accuracy: 0.7139 - val_loss: 0.6307 - val_precision: 0.7020 - val_recall: 0.7596\n",
      "Epoch 167/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8203 - loss: 0.3908 - precision: 0.8096 - recall: 0.8450 - val_accuracy: 0.7094 - val_loss: 0.6296 - val_precision: 0.7030 - val_recall: 0.7414\n",
      "Epoch 168/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8226 - loss: 0.3926 - precision: 0.8121 - recall: 0.8478 - val_accuracy: 0.7130 - val_loss: 0.6320 - val_precision: 0.7007 - val_recall: 0.7598\n",
      "Epoch 169/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8224 - loss: 0.3897 - precision: 0.8132 - recall: 0.8447 - val_accuracy: 0.7102 - val_loss: 0.6345 - val_precision: 0.6979 - val_recall: 0.7576\n",
      "Epoch 170/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8205 - loss: 0.3890 - precision: 0.8116 - recall: 0.8413 - val_accuracy: 0.7144 - val_loss: 0.6323 - val_precision: 0.7089 - val_recall: 0.7434\n",
      "Epoch 171/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8191 - loss: 0.3906 - precision: 0.8108 - recall: 0.8409 - val_accuracy: 0.7129 - val_loss: 0.6336 - val_precision: 0.7180 - val_recall: 0.7164\n",
      "Epoch 172/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8180 - loss: 0.3933 - precision: 0.8101 - recall: 0.8410 - val_accuracy: 0.7152 - val_loss: 0.6353 - val_precision: 0.7098 - val_recall: 0.7437\n",
      "Epoch 173/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8197 - loss: 0.3910 - precision: 0.8105 - recall: 0.8400 - val_accuracy: 0.7134 - val_loss: 0.6365 - val_precision: 0.7076 - val_recall: 0.7433\n",
      "Epoch 174/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8197 - loss: 0.3915 - precision: 0.8107 - recall: 0.8394 - val_accuracy: 0.7096 - val_loss: 0.6349 - val_precision: 0.7001 - val_recall: 0.7500\n",
      "Epoch 175/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8225 - loss: 0.3908 - precision: 0.8125 - recall: 0.8449 - val_accuracy: 0.7119 - val_loss: 0.6344 - val_precision: 0.7101 - val_recall: 0.7321\n",
      "Epoch 176/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8223 - loss: 0.3874 - precision: 0.8147 - recall: 0.8429 - val_accuracy: 0.7065 - val_loss: 0.6362 - val_precision: 0.7053 - val_recall: 0.7258\n",
      "Epoch 177/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8208 - loss: 0.3861 - precision: 0.8147 - recall: 0.8409 - val_accuracy: 0.7089 - val_loss: 0.6367 - val_precision: 0.7057 - val_recall: 0.7328\n",
      "Epoch 178/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.3902 - precision: 0.8090 - recall: 0.8392 - val_accuracy: 0.7081 - val_loss: 0.6383 - val_precision: 0.7164 - val_recall: 0.7045\n",
      "Epoch 179/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8160 - loss: 0.3920 - precision: 0.8088 - recall: 0.8363 - val_accuracy: 0.7154 - val_loss: 0.6401 - val_precision: 0.7111 - val_recall: 0.7411\n",
      "Epoch 180/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8197 - loss: 0.3893 - precision: 0.8098 - recall: 0.8424 - val_accuracy: 0.6988 - val_loss: 0.6453 - val_precision: 0.7180 - val_recall: 0.6708\n",
      "Epoch 181/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8220 - loss: 0.3868 - precision: 0.8143 - recall: 0.8410 - val_accuracy: 0.7105 - val_loss: 0.6404 - val_precision: 0.7067 - val_recall: 0.7360\n",
      "Epoch 182/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8194 - loss: 0.3877 - precision: 0.8119 - recall: 0.8409 - val_accuracy: 0.7142 - val_loss: 0.6466 - val_precision: 0.6990 - val_recall: 0.7687\n",
      "Epoch 183/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8223 - loss: 0.3856 - precision: 0.8122 - recall: 0.8444 - val_accuracy: 0.7133 - val_loss: 0.6463 - val_precision: 0.7023 - val_recall: 0.7568\n",
      "Epoch 184/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.3863 - precision: 0.8149 - recall: 0.8410 - val_accuracy: 0.7133 - val_loss: 0.6493 - val_precision: 0.7028 - val_recall: 0.7555\n",
      "Epoch 185/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8245 - loss: 0.3826 - precision: 0.8147 - recall: 0.8467 - val_accuracy: 0.7152 - val_loss: 0.6536 - val_precision: 0.7020 - val_recall: 0.7639\n",
      "Epoch 186/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8233 - loss: 0.3851 - precision: 0.8161 - recall: 0.8420 - val_accuracy: 0.7122 - val_loss: 0.6478 - val_precision: 0.7061 - val_recall: 0.7431\n",
      "Epoch 187/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.3834 - precision: 0.8142 - recall: 0.8437 - val_accuracy: 0.7154 - val_loss: 0.6493 - val_precision: 0.7141 - val_recall: 0.7340\n",
      "Epoch 188/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8236 - loss: 0.3860 - precision: 0.8158 - recall: 0.8456 - val_accuracy: 0.7111 - val_loss: 0.6493 - val_precision: 0.7003 - val_recall: 0.7546\n",
      "Epoch 189/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8265 - loss: 0.3786 - precision: 0.8185 - recall: 0.8476 - val_accuracy: 0.7072 - val_loss: 0.6514 - val_precision: 0.7124 - val_recall: 0.7107\n",
      "Epoch 190/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3800 - precision: 0.8166 - recall: 0.8512 - val_accuracy: 0.7111 - val_loss: 0.6518 - val_precision: 0.7152 - val_recall: 0.7170\n",
      "Epoch 191/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8240 - loss: 0.3803 - precision: 0.8162 - recall: 0.8430 - val_accuracy: 0.7104 - val_loss: 0.6487 - val_precision: 0.7155 - val_recall: 0.7143\n",
      "Epoch 192/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8242 - loss: 0.3841 - precision: 0.8163 - recall: 0.8447 - val_accuracy: 0.7068 - val_loss: 0.6515 - val_precision: 0.7137 - val_recall: 0.7067\n",
      "Epoch 193/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8235 - loss: 0.3800 - precision: 0.8155 - recall: 0.8422 - val_accuracy: 0.7103 - val_loss: 0.6567 - val_precision: 0.6917 - val_recall: 0.7755\n",
      "Epoch 194/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.3810 - precision: 0.8167 - recall: 0.8405 - val_accuracy: 0.7063 - val_loss: 0.6552 - val_precision: 0.7021 - val_recall: 0.7334\n",
      "Epoch 195/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8269 - loss: 0.3794 - precision: 0.8196 - recall: 0.8463 - val_accuracy: 0.7097 - val_loss: 0.6540 - val_precision: 0.7156 - val_recall: 0.7117\n",
      "Epoch 196/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8241 - loss: 0.3814 - precision: 0.8174 - recall: 0.8434 - val_accuracy: 0.7136 - val_loss: 0.6549 - val_precision: 0.7037 - val_recall: 0.7540\n",
      "Epoch 197/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8225 - loss: 0.3812 - precision: 0.8117 - recall: 0.8436 - val_accuracy: 0.7075 - val_loss: 0.6589 - val_precision: 0.7167 - val_recall: 0.7022\n",
      "Epoch 198/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8227 - loss: 0.3824 - precision: 0.8149 - recall: 0.8426 - val_accuracy: 0.7095 - val_loss: 0.6555 - val_precision: 0.7088 - val_recall: 0.7271\n",
      "Epoch 199/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8267 - loss: 0.3790 - precision: 0.8188 - recall: 0.8447 - val_accuracy: 0.7113 - val_loss: 0.6583 - val_precision: 0.7055 - val_recall: 0.7415\n",
      "Epoch 200/200\n",
      "\u001b[1m1511/1511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3792 - precision: 0.8211 - recall: 0.8455 - val_accuracy: 0.7058 - val_loss: 0.6567 - val_precision: 0.7105 - val_recall: 0.7105\n"
     ]
    }
   ],
   "source": [
    "model = create_diabetes_model(X_train_scaled.shape[1])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02a7329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m648/648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 776us/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70     10188\n",
      "           1       0.71      0.71      0.71     10530\n",
      "\n",
      "    accuracy                           0.71     20718\n",
      "   macro avg       0.71      0.71      0.71     20718\n",
      "weighted avg       0.71      0.71      0.71     20718\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7140 3048]\n",
      " [3048 7482]]\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluate\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a15a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
